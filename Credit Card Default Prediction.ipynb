{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSA3020 - Assignment 2  \n",
    "\n",
    "**Assignment Points**: 100  \n",
    "**Assignment Weight**: 20%  \n",
    "**Submission**: Provide your answers in this notebook and submit it via iLearn link\n",
    "\n",
    "- Do NOT use `print()` in the final version of the assignment unless you are explicitly asked to do so\n",
    "- Make sure your answers are direct and easy to read\n",
    "- Marks will be deducted for not following instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1** - Reading and describing data (Total Marks: 20)\n",
    "\n",
    "\n",
    "\n",
    "**Q1**. Read the **modified** credit card dataset provided in the **assignment_data** folder contained in the assignment zip file\n",
    "- Name your DataFrame `df`  \n",
    "- Rename the columns 'PAY_0' and 'default payment next month' as in Programming Task 1 \n",
    "- Delete 'ID' column  \n",
    "\n",
    "(5 marks) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>payment_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3913.0</td>\n",
       "      <td>3102.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29239.0</td>\n",
       "      <td>14027.0</td>\n",
       "      <td>13559.0</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46990.0</td>\n",
       "      <td>48233.0</td>\n",
       "      <td>49291.0</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8617.0</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>35835.0</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  LIMIT_BAL  SEX  EDUCATION  MARRIAGE   AGE  PAY_1  PAY_2  PAY_3  \\\n",
       "0           0    20000.0  2.0        2.0       1.0  24.0    2.0    2.0   -1.0   \n",
       "1           1   120000.0  2.0        2.0       2.0  26.0   -1.0    2.0    0.0   \n",
       "2           2    90000.0  2.0        2.0       2.0  34.0    0.0    0.0    0.0   \n",
       "3           3    50000.0  2.0        2.0       1.0  37.0    0.0    0.0    0.0   \n",
       "4           4    50000.0  1.0        2.0       1.0  57.0   -1.0    0.0   -1.0   \n",
       "\n",
       "   PAY_4  PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0   -1.0   -2.0   -2.0     3913.0     3102.0      689.0        0.0        0.0   \n",
       "1    0.0    0.0    2.0     2682.0     1725.0     2682.0     3272.0     3455.0   \n",
       "2    0.0    0.0    0.0    29239.0    14027.0    13559.0    14331.0    14948.0   \n",
       "3    0.0    0.0    0.0    46990.0    48233.0    49291.0    28314.0    28959.0   \n",
       "4    0.0    0.0    0.0     8617.0     5670.0    35835.0    20940.0    19146.0   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0        0.0       0.0     689.0       0.0       0.0       0.0         0   \n",
       "1     3261.0       0.0    1000.0    1000.0    1000.0       0.0      2000   \n",
       "2    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0      5000   \n",
       "3    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0      1000   \n",
       "4    19131.0    2000.0   36681.0   10000.0    9000.0     689.0       679   \n",
       "\n",
       "   payment_default  \n",
       "0                1  \n",
       "1                1  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, \"display.width\", None)\n",
    "df = pd.read_excel('assignment_data/credit_card_defaults.xlsx')\n",
    "\n",
    "df.rename(columns={'default payment next month': 'payment_default'}, inplace = True)\n",
    "df.rename(columns={'PAY_0': 'PAY_1'}, inplace = True)\n",
    "\n",
    "df.drop(\"ID\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. Explain which features are numeric, ordinal, and nominal variables, and how many features of each variable type there are in the dataset\n",
    "- To answer this question look at the data itself and consider the definitions of the variables (provided elsewhere in course material)\n",
    "- Your final answer should consist of text only and not contain any python code\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LIMIT_BAL** is a numerical variable, with minimum at 10,000 to maximum of 1,000,000\n",
    "- **SEX** is a nominal variable, where 1 = male and 2 = female (2 features)  \n",
    "- **EDUCATION** is a nominal variable, where the values 1-6 represent the level of education (1 = graduate school, 3 = high-school, 2 = University, 4 = Other, 5 = unknown, 6 = unknown) (6 features)  \n",
    "- **MARRIAGE** is a nominal variable, represented by 4 values (1 = married, 2 = single, 3 = other, 4 = unknown)    \n",
    "- **AGE** is a discrete numeric variable\n",
    "- **PAY_1-6** is a ordinal variable, representing repayment status in months \n",
    "- **BILL_AMOUNT 1-6** is a continuous numerical variable, representing bill statement in dollars  \n",
    "- **PAY_AMOUNT1-6** is a continuous numerical variable, representing previous payment in dollars    \n",
    "- **PAYMENT_DEFAULT** is a nominal variable where 1 = yes and 0 = no (2 features)  \n",
    "\n",
    "**Numerical** - 14\n",
    "**Ordinal** - 6\n",
    "**Nominal** - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3.** Describing the dataset. \n",
    "\n",
    "- Print `info()` and comment on the missing values in the dataset\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10001 entries, 0 to 10000\n",
      "Data columns (total 25 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       10001 non-null  int64  \n",
      " 1   LIMIT_BAL        9993 non-null   float64\n",
      " 2   SEX              9993 non-null   float64\n",
      " 3   EDUCATION        9983 non-null   float64\n",
      " 4   MARRIAGE         9983 non-null   float64\n",
      " 5   AGE              9983 non-null   float64\n",
      " 6   PAY_1            9983 non-null   float64\n",
      " 7   PAY_2            9983 non-null   float64\n",
      " 8   PAY_3            9971 non-null   float64\n",
      " 9   PAY_4            9964 non-null   float64\n",
      " 10  PAY_5            9964 non-null   float64\n",
      " 11  PAY_6            9971 non-null   float64\n",
      " 12  BILL_AMT1        9971 non-null   float64\n",
      " 13  BILL_AMT2        9979 non-null   float64\n",
      " 14  BILL_AMT3        9979 non-null   float64\n",
      " 15  BILL_AMT4        9979 non-null   float64\n",
      " 16  BILL_AMT5        9979 non-null   float64\n",
      " 17  BILL_AMT6        9979 non-null   float64\n",
      " 18  PAY_AMT1         9969 non-null   float64\n",
      " 19  PAY_AMT2         9969 non-null   float64\n",
      " 20  PAY_AMT3         9969 non-null   float64\n",
      " 21  PAY_AMT4         9969 non-null   float64\n",
      " 22  PAY_AMT5         9981 non-null   float64\n",
      " 23  PAY_AMT6         10001 non-null  int64  \n",
      " 24  payment_default  10001 non-null  int64  \n",
      "dtypes: float64(22), int64(3)\n",
      "memory usage: 1.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, as well as the count of columns which can be represented in column 1 (unnamed) which acts as an index, we can see there are 10001 rows altogether. It also states that there are 10001 entries. Looking at each column from LIMIT_BAL to PAY_AMT5, we can see there are missing values as it does not equal to the number of rows. Alternatively, we can see that PAY_AMT6 and payment_default both have 10001 which means there are no missing values for those columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 2.** Cleaning data and dealing with categorical features (Total Marks: 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** \n",
    "- Use an appropriate `pandas` function to impute missing values (10 marks)\n",
    "    - Be careful when deciding which method to use to replace missing observations \n",
    "    - Take into consideration the type of each variable and the best practices we discussed in class/lecture notes\n",
    "- Explain what you have done and why (5 marks)\n",
    "\n",
    "(Total: 15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode\n",
    "df['SEX'].fillna(df['SEX'].mode()[0], inplace=True)\n",
    "df['EDUCATION'].fillna(df['EDUCATION'].mode()[0], inplace=True)\n",
    "df['MARRIAGE'].fillna(df['MARRIAGE'].mode()[0], inplace=True) \n",
    "df['PAY_1'].fillna(df['PAY_1'].mode()[0], inplace=True)\n",
    "df['PAY_2'].fillna(df['PAY_2'].mode()[0], inplace=True)\n",
    "df['PAY_3'].fillna(df['PAY_3'].mode()[0], inplace=True)\n",
    "df['PAY_4'].fillna(df['PAY_4'].mode()[0], inplace=True)\n",
    "df['PAY_5'].fillna(df['PAY_5'].mode()[0], inplace=True)\n",
    "df['PAY_6'].fillna(df['PAY_6'].mode()[0], inplace=True)\n",
    "\n",
    "# Mean\n",
    "df['AGE'].fillna(df['AGE'].mean(), inplace=True) \n",
    "df['LIMIT_BAL'].fillna(df['LIMIT_BAL'].mean(), inplace=True)\n",
    "df['BILL_AMT1'].fillna(df['BILL_AMT1'].mean(), inplace=True)\n",
    "df['BILL_AMT2'].fillna(df['BILL_AMT2'].mean(), inplace=True)\n",
    "df['BILL_AMT3'].fillna(df['BILL_AMT3'].mean(), inplace=True)\n",
    "df['BILL_AMT4'].fillna(df['BILL_AMT4'].mean(), inplace=True)\n",
    "df['BILL_AMT5'].fillna(df['BILL_AMT5'].mean(), inplace=True)\n",
    "df['BILL_AMT6'].fillna(df['BILL_AMT6'].mean(), inplace=True)\n",
    "df['PAY_AMT1'].fillna(df['PAY_AMT1'].mean(), inplace=True)\n",
    "df['PAY_AMT2'].fillna(df['PAY_AMT2'].mean(), inplace=True)\n",
    "df['PAY_AMT3'].fillna(df['PAY_AMT3'].mean(), inplace=True)\n",
    "df['PAY_AMT4'].fillna(df['PAY_AMT4'].mean(), inplace=True)\n",
    "df['PAY_AMT5'].fillna(df['PAY_AMT5'].mean(), inplace=True)\n",
    "df['PAY_AMT6'].fillna(df['PAY_AMT6'].mean(), inplace=True)\n",
    "\n",
    "#print('df.isnull().sum()\\n',df.isnull().sum())\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no repetition of the index that is trying to be replaced, there is no simpler way to replace all the the NaN values above. If there was, a loop function may have been used. \n",
    "\n",
    "For the data, either the column mode or mean was used for each column. The mode functionwas used for SEX, EDUCATION, MARRIAGE and PAY_1-6 as the numbers had an associated categorical label attached to it. The mode is the most common value in their respective column.\n",
    "\n",
    "Furthermore, the mean function has been used for all numerical values being BILL_AMT1-6, PAY_AMT1-6, LIMIT_BAL and AGE which are not encoded to a categorical value and are simply infinitely large values. As our data does not showcase many or significant outliers, it is prefered to use the mean imputation method over the median. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "- Print `value_counts()` of 'SEX' column and add dummy variables 'SEX_MALE' and 'SEX_FEMALE' to the `df` using `get_dummies()` (3 marks)\n",
    "- Carefully explain how the dummy variables are constructed (1 mark)\n",
    "- Delete the variable 'SEX' from `df` (1 mark)\n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEX\n",
      "2.0    5799\n",
      "1.0    4202\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Value count\n",
    "print(df.value_counts(\"SEX\"))\n",
    "\n",
    "#Dummy Variable creation\n",
    "df['SEX'] = df['SEX'].apply(str)\n",
    "SEX_dummies = pd.get_dummies(df['SEX'])\n",
    "df = df.join(SEX_dummies)\n",
    "df.rename(columns = {\"1.0\" : \"SEX_MALE\", \"2.0\" : \"SEX_FEMALE\"},inplace = True)\n",
    "\n",
    "#Delete SEX\n",
    "del df[\"SEX\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy variables indicates whether a separate categorical variable takes on a specific value.\n",
    "The initial sex column was comprised of 1.0 for Males and 2.0 for Females. To ensure we can look at these independently, we create dummy variables which are two additional columns which take on a 0 or 1 for whether they were Male or Female. As the row can only be a male or female, the value will be 1 for the actual value and 0 for the alternative choice. Nevertheless, this ensures our future models dont think that FEMALE > MALE in which would result in accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. Print `value_counts()` of 'MARRIAGE' column and *carefully* comment on what you notice in relation to the definition of this variable. \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0    5476\n",
      "1.0    4383\n",
      "3.0     125\n",
      "0.0      17\n",
      "Name: MARRIAGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['MARRIAGE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the data, it is apparent that most records are individuals who are single with 5458 with slightly less people who are married at 4383 recorded values. A very small proportion of people selected 'Other' as an option at just 125. Finally we can see that 17 records were recorded with a value of 0 which is not described in the data. This may be mistyped responses, data that was lost in mannual transfer or fields that were chosen not to be filled. Moving forward, this data should be reallocated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q4**. \n",
    "\n",
    "- Apply `get_dummies()` to 'MARRIAGE' feature and add dummy variables 'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHER' to `df`. (2.5 marks)   \n",
    "- Carefully consider how to allocate all values of 'MARRIAGE' across the 3 newly created features (5 marks)   \n",
    "    - Explain what decisions you had to make\n",
    "- Delete 'MARRIAGE' from `df` (2.5 marks)   \n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 Value\n",
    "df[\"MARRIAGE\"].replace([0],3,inplace=True)\n",
    "\n",
    "#Dummy variable creation\n",
    "df['MARRIAGE'] = df['MARRIAGE'].apply(str)\n",
    "MARRIAGE_dummies = pd.get_dummies(df['MARRIAGE'])\n",
    "df = df.join(MARRIAGE_dummies)\n",
    "df.rename(columns = {'1.0': 'MARRIAGE_MARRIED', '2.0': 'MARRIAGE_SINGLE', '3.0': 'MARRIAGE_OTHER'},inplace = True)\n",
    "\n",
    "#Delete Marriage\n",
    "del df[\"MARRIAGE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is only a small number of value 0.0, we can assume the user inputted a response that would fit under other but they gave a specific response and did not pick other. For this reason, I have decided to replace the 0s with 3, which will put it into the OTHER column. Therefore, it is more suitable to put it into a pre-existing column that better suits the data as other, in comparison to putting it into a column which we certainly know it doesnt belong to. By implementing this prior to the get_dummies function, it resulted in no unwanted column with 0s. Furthermore, the MARRIGE column is dropped automaticaly when it is converted to the dummy variables and therefore does not need to be deleted mannualy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q5**. In the column 'EDUCATION', convert the values {0, 4, 5, 6} into the value 4. \n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EDUCATION\"].replace([0,4,5,6],4, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 3** Preparing X and y arrays (Total Marks: 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Create `y` from the first 10,000 observations of 'payment_default' column from `df` (2.5 marks)   \n",
    "- Create `X`  from the first 10,000 observations of all the remaining features in `df` (2.5 marks)   \n",
    "\n",
    "(Total: 5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = df['payment_default'].iloc[:10000].values\n",
    "X = df.drop(['payment_default'], axis = 1).iloc[:10000].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "\n",
    "- Use an appropriate scikit-learn library we used in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% train and 30% test datasets (2.5 marks) \n",
    "    - Set random_state to 4 and stratify the subsamples so that train and test datasets have roughly equal proportions of the target's class labels \n",
    "- Standardise the data using `StandardScaler` library (2.5 marks)   \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4, stratify = y)\n",
    "\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 4**. Support Vector Classifier and Accuracies (Total Marks: 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Train a Support Vector Classifier on standardised data (5 marks)\n",
    "    - Use `linear` kernel and set `random_state` to 3 (don't change any other parameters)\n",
    "- Compute and print training and test dataset accuracies (5 marks)\n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Training dataset 0.7816\n",
      "Accuracy - Test dataset 0.7843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear',random_state=3)\n",
    "\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "svm.fit(X_test_scaled, y_test)\n",
    "\n",
    "print(f'Accuracy - Training dataset {svm.score(X_train_scaled, y_train):.4f}')\n",
    "print(f'Accuracy - Test dataset {svm.score(X_test_scaled, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2.**\n",
    "\n",
    "- Extract 3 linear principal components from the standardised features using an appropriate `sklearn` library (5 marks)\n",
    "- Train a Support Vector Classifier on the 3 principal components computed above (5 marks)   \n",
    "    - Use `linear` kernel and set `random_state` to 3 (don't change any other parameters)\n",
    "- Compute and print training and test dataset accuracies (5 marks)\n",
    "\n",
    "\n",
    "(Total: 15 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Training dataset 0.7741\n",
      "Accuracy - Test dataset 0.7740\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#PCA\n",
    "svm_PCA = SVC(kernel='linear',random_state=3)\n",
    "pca = PCA(n_components = 3)\n",
    "\n",
    "#Transforming & Fitting\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "svm_PCA.fit(X_train_pca, y_train)\n",
    "\n",
    "#Accuracy\n",
    "print(f'Accuracy - Training dataset {svm_PCA.score(X_train_pca, y_train):.4f}')\n",
    "print(f'Accuracy - Test dataset {svm_PCA.score(X_test_pca, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. \n",
    "\n",
    "- Compare and comment on the computed accuracies from the last two questions above\n",
    "    - Make comparisons both within and across the two questions\n",
    "\n",
    "(Total: 5 marks)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the original model that has no dimension reduction, we can see the training and test accuracies have values that are very close to one another with 0.7816 for training and 0.7843 for testing. As the accuracy between the training and testing set are similar (<1%), we can assume that the models are not overfitting the data.\n",
    "\n",
    "Altnernatively, the SVM model which uses PCA (n = 3) as a dimensional reduction method contains a training and testing accuracy score of 0.7741 and 0.7740 respectively. As the accuracy between the training and testing set are similar, we can assume that the models are not overfitting the data.\n",
    "\n",
    "\n",
    "When comparing the two models, we can see that by utilising dimensional reduction methods like PCA, in this case it does not improve the accuracy of our support vector machine. This may be as a result of creating components which do not capture the majority of the variance in the data, and therefore, there is important features that is being excluded from this model. This is due to the model not consideirng the variable/prediction target, and only focuses on the variance. Therefore, the features which may have large variance may have nothing to do with the prediction value or PAYMENT_DEFAULT. As a result, possibly more principal components should be used. Hence, we may assume that the second model has a worse bias-variance tradeoff in comparison to the original model. Consequently, we believe model 1 fits the data better and predicts the classifcation correctly more often."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
